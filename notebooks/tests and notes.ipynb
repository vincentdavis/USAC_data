{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "hdrs = {'User-Agent':'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'}\n",
    "r = requests.session()\n",
    "\n",
    "# This is the results page, Need to scrape the columns;\n",
    "# Place, Points, Name, City, State, Time, USAC#, Bib, Team\n",
    "# json is http://www.usacycling.org/results/index.php?ajax=1&act=infoid&info_id=94310\n",
    "raceresults = r.get('http://www.usacycling.org/results/index.php?year=2016&id=2', headers=hdrs)\n",
    "race = BeautifulSoup(raceresults.text, 'html.parser')\n",
    "\n",
    "# This is the details of lap times for the race. need to scrape the columns\n",
    "# Place Name Lap 1 Lap 2 Lap 3 Lap 4   each race may have different number of laps, more or less than 4\n",
    "racesplits = r.get('https://www.usacycling.org/results/index.php?ajax=1&act=loadresults&race_id=955427', headers=hdrs)\n",
    "p=json.loads(racesplits.text)['message']\n",
    "BeautifulSoup(p, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "hdrs = {'User-Agent':'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'}\n",
    "r = requests.session()\n",
    "raceresults = r.get('http://www.usacycling.org/results/index.php?year=2016&id=2', headers=hdrs)\n",
    "for e in range(94310, 94311): # these are event id's\n",
    "    response = r.get('http://www.usacycling.org/results/index.php?ajax=1&act=infoid&info_id='+str(e), headers=hdrs)\n",
    "    message = json.loads(response.text)['message']\n",
    "    race = BeautifulSoup(message, 'html.parser')\n",
    "    if \"No results found.\" not in race:\n",
    "        print('Race is: {} and race name is: {}'.format(e, ' | '.join(x for x in race.find('h3').find_all(text=True))))\n",
    "        for a in race.find_all('li'):\n",
    "            print('---- {}'.format(a.find('a').contents[0])) # get the text from the href\n",
    "            print('    ---- {}'.format(a.get('id'))) # get the id tage value\n",
    "            results = r.get('https://www.usacycling.org/results/index.php?ajax=1&act=loadresults&race_id=955427', headers=hdrs)\n",
    "            splits = r.get('https://www.usacycling.org/results/index.php?ajax=1&act=splits&race_id=955427', headers=hdrs)  \n",
    "    else:\n",
    "        print('No event: {}'.format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parses results table\n",
    "#race\n",
    "rm = json.loads(results.text)['message']\n",
    "r = BeautifulSoup(sm, 'html.parser')\n",
    "# Header\n",
    "for h in r.find_all('div', attrs={'class':\"tablecell header\"}):\n",
    "    if len(h.get_text()) > 1:\n",
    "        print(h.get_text())\n",
    "# Results\n",
    "for h in r.find_all('div', attrs={\"class\":\"tablerow\"}):\n",
    "    print(\"-------------------\")\n",
    "    print(' | '.join(c.get_text() for c in h.find_all('div', attrs={\"class\":\"tablecell results\"})))\n",
    "\n",
    "# x[1].find_all('div')[4].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = BeautifulSoup(splits.text, 'html.parser')\n",
    "for r in s.find_all('tr'):\n",
    "    if 'PlaceNameLap' in r.get_text():\n",
    "        print(' | '.join(c.get_text() for c in r.find_all('th')))\n",
    "    print(' | '.join(c.get_text() for c in r.find_all('td')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = race.find_all(id)\n",
    "for l in x.find_all('li'):\n",
    "    print(l.get('id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in x.find_all('a'):\n",
    "    print(a.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDRS = {'User-Agent':'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'}\n",
    "r = requests.session()\n",
    "# We have to load this page first or other pages return unauthorized access\n",
    "startpage = r.get('http://www.usacycling.org/events/rr.php', headers=HDRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startpage.cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.session()\n",
    "bool(r.cookies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "HDRS = {'User-Agent':'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'}\n",
    "r = requests.session()\n",
    "\n",
    "# This is the results page, Need to scrape the columns;\n",
    "# Place, Points, Name, City, State, Time, USAC#, Bib, Team\n",
    "# json is http://www.usacycling.org/results/index.php?ajax=1&act=infoid&info_id=94310\n",
    "raceresults = r.get('http://www.usacycling.org/results/index.php?year=2016&id=2', headers=HDRS)\n",
    "#race = BeautifulSoup(raceresults.text, 'html.parser')\n",
    "\n",
    "def get_event_list(session, year):\n",
    "    \"\"\"\n",
    "\n",
    "    :param year:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    eventspage = r.get(\"http://www.usacycling.org/events/?zipcode=80439&radius=50000&race=&fyear=\" + str(year) + \"&rrfilter=rr\" , headers=HDRS)\n",
    "    soup = BeautifulSoup(eventspage.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "s = get_event_list(r, 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in s.find('table').find_all('tr'):  # for each row (event)\n",
    "    if 'Permit Number:' in row.get_text(): # Skip the header row\n",
    "        cells = row.find_all('td') # rows are devided into 3 columns\n",
    "        #print(cells[1].find('b')) # event Name\n",
    "        #print(cells[1].find(text = re.compile(\"\\s+Permit Number:\\s\\S+\")))\n",
    "        #print(cells[1].find(text = re.compile(\"\\s+\\d{2}/\\d{2}/\\d{4}\")))\n",
    "        #print(cells[1].find('a', href=True, text='Event Flyer')['href']) \n",
    "        print(cells[1].find('a', href=True, text='Event Website')['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = s.find('table').find('tr')  # Search result row\n",
    "# print('*** {}'.format(row.get_text()))\n",
    "row = row.find_next_sibling() # Column header row\n",
    "# print('*** {}'.format(row.get_text()))\n",
    "# if 'Permit Number:' in row.get_text(): # Skip the header row\n",
    "#     cells = row.find_all('td') # rows are devided into 3 columns\n",
    "#     #print(cells[1].find('b')) # event Name\n",
    "#     print(cells[1].find(text = re.compile(\"\\s+Permit Number:\\s\\S+\")))\n",
    "#     print(cells[1].find(text = re.compile(\"\\s+\\d{2}/\\d{2}/\\d{4}\")))\n",
    "#     #print(cells[1].find('a', href=True, text='Event Flyer')) \n",
    "#     #print(cells[1].find('a', href=True, text='Event Website'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from usac import *\n",
    "from db import DB, DB_INIT\n",
    "#DB_INIT(remove=True)\n",
    "DB.connect() \n",
    "for y in range(2005, 2017):\n",
    "    get_event_list(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(3,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}